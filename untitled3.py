# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gGhcL0DO9LesFaiU9O8UkuY9RJr3ecag
"""

# Ransomware Analysis Framework for International and EU Cybercrime Instruments
# This notebook demonstrates how to use the Ransomware Analysis Framework

# Step 1: Import necessary libraries and framework
import pandas as pd
import re
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import networkx as nx
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# --- RANSOMWARE FRAMEWORK CODE ---
class RansomwareFrameworkAnalyzer:
    """
    A comprehensive framework for analyzing ransomware framing in international
    and EU cybercrime instruments.
    """

    def __init__(self):
        """Initialize the analyzer with the legal instruments and their metadata."""
        self.documents = {
            'budapest': {
                'title': 'Convention on Cybercrime (Budapest Convention)',
                'year': 2001,
                'body': 'Council of Europe',
                'type': 'International Convention',
                'text': None
            },
            'directive_attacks': {
                'title': 'Directive on Attacks Against Information Systems (2013/40/EU)',
                'year': 2013,
                'body': 'European Union',
                'type': 'EU Directive',
                'text': None
            },
            'ransomware_guidance': {
                'title': 'T-CY Guidance Note #12: Aspects of Ransomware covered by the Budapest Convention',
                'year': 2022,
                'body': 'Cybercrime Convention Committee (T-CY)',
                'type': 'Guidance Note',
                'text': None
            },
            'nis2': {
                'title': 'Directive on measures for a high common level of cybersecurity across the Union (NIS 2 Directive)',
                'year': 2022,
                'body': 'European Union',
                'type': 'EU Directive',
                'text': None
            }
        }

        # Define framing categories for analysis
        self.framing_categories = {
            'technical': ['malware', 'encryption', 'data', 'system', 'computer', 'technical', 'decrypt',
                          'software', 'access', 'network', 'internet', 'digital', 'botnet', 'code'],

            'criminal': ['offence', 'criminal', 'illegal', 'fraud', 'extortion', 'blackmail', 'threat',
                         'unauthorized', 'crime', 'perpetrator', 'offender', 'attack', 'illicit'],

            'economic': ['economic', 'financial', 'payment', 'cost', 'damage', 'ransom', 'money',
                         'cryptocurrency', 'bitcoin', 'market', 'loss', 'business', 'profit', 'economy'],

            'governance': ['regulation', 'directive', 'law', 'policy', 'compliance', 'governance',
                           'authority', 'jurisdiction', 'state', 'enforcement', 'cooperation', 'international'],

            'security': ['security', 'protection', 'defense', 'safeguard', 'risk', 'threat', 'vulnerability',
                         'resilience', 'prevention', 'detection', 'response', 'mitigation'],

            'societal': ['society', 'public', 'citizen', 'impact', 'harm', 'critical', 'infrastructure',
                         'service', 'essential', 'health', 'safety', 'privacy', 'social', 'human'],

            'operational': ['investigation', 'evidence', 'procedure', 'measure', 'prosecution', 'judicial',
                           'technical', 'operational', 'cooperation', 'assistance', 'exchange', 'information'],

            'evolutionary': ['evolve', 'develop', 'emerge', 'advance', 'sophisticate', 'complexity',
                            'trend', 'increase', 'growth', 'change', 'adapt', 'professional', 'organized']
        }

        # Define ransomware-specific terms and concepts
        self.ransomware_terms = [
            'ransom', 'ransomware', 'extortion', 'encrypt', 'decrypt', 'payment',
            'cryptocurrency', 'bitcoin', 'restore', 'access', 'lock', 'malware',
            'blackmail', 'crypto', 'demand', 'pay', 'hostage', 'data', 'system',
            'restore', 'recovery', 'key', 'unlock', 'victim', 'attack'
        ]

        # Article and recital patterns for extraction
        self.article_patterns = {
            'budapest': r'Article\s+(\d+)\s+[–-]\s+([^\n]+)',
            'directive_attacks': r'Article\s+(\d+)\s+[–-]\s+([^\n]+)',
            'ransomware_guidance': r'(\d+)\s+([A-Za-z\s]+)',
            'nis2': r'Article\s+(\d+)\s+[–-]\s+([^\n]+)'
        }

        self.recital_patterns = {
            'budapest': None,  # Budapest Convention doesn't have recitals in the same way
            'directive_attacks': r'\((\d+)\)\s+([^.]+)',
            'ransomware_guidance': None,  # Guidance note doesn't have recitals
            'nis2': r'\((\d+)\)\s+([^.]+)'
        }

        # Results
        self.framing_results = {}
        self.articles_references = {}
        self.recitals_references = {}
        self.temporal_evolution = {}
        self.cross_document_comparison = {}

    def load_document(self, doc_id, text):
        """Load document text into the analyzer."""
        if doc_id in self.documents:
            self.documents[doc_id]['text'] = text
            print(f"Loaded {self.documents[doc_id]['title']} ({self.documents[doc_id]['year']})")
            return True
        else:
            print(f"Unknown document ID: {doc_id}")
            return False

    def extract_articles(self, doc_id):
        """Extract articles from a document."""
        if doc_id not in self.documents or not self.documents[doc_id]['text']:
            print(f"Document {doc_id} not loaded")
            return {}

        text = self.documents[doc_id]['text']
        pattern = self.article_patterns.get(doc_id)

        if not pattern:
            return {}

        articles = {}
        matches = re.finditer(pattern, text)

        for match in matches:
            if len(match.groups()) >= 2:
                article_num = match.group(1)
                article_title = match.group(2).strip()

                # Find the article content
                start_pos = match.end()

                # Find the next article or the end of text
                next_match = re.search(r'Article\s+\d+', text[start_pos:])
                if next_match:
                    end_pos = start_pos + next_match.start()
                else:
                    end_pos = len(text)

                article_content = text[start_pos:end_pos].strip()
                articles[article_num] = {
                    'title': article_title,
                    'content': article_content
                }

        print(f"Extracted {len(articles)} articles from {self.documents[doc_id]['title']}")
        return articles

    def extract_recitals(self, doc_id):
        """Extract recitals from a document."""
        if doc_id not in self.documents or not self.documents[doc_id]['text']:
            print(f"Document {doc_id} not loaded")
            return {}

        text = self.documents[doc_id]['text']
        pattern = self.recital_patterns.get(doc_id)

        if not pattern:
            return {}

        recitals = {}
        matches = re.finditer(pattern, text)

        for match in matches:
            if len(match.groups()) >= 2:
                recital_num = match.group(1)
                recital_content = match.group(2).strip()
                recitals[recital_num] = recital_content

        print(f"Extracted {len(recitals)} recitals from {self.documents[doc_id]['title']}")
        return recitals

    def analyze_framing(self, doc_id):
        """Analyze how ransomware is framed in a specific document."""
        if doc_id not in self.documents or not self.documents[doc_id]['text']:
            print(f"Document {doc_id} not loaded")
            return {}

        text = self.documents[doc_id]['text'].lower()
        results = {}

        # Analyze presence of framing categories
        for category, terms in self.framing_categories.items():
            category_count = 0
            category_matches = []

            for term in terms:
                pattern = r'\b' + re.escape(term) + r'[a-z]*\b'
                matches = re.finditer(pattern, text)

                for match in matches:
                    category_count += 1
                    category_matches.append(match.group(0))

            results[category] = {
                'count': category_count,
                'matches': Counter(category_matches).most_common()
            }

        # Analyze presence of ransomware-specific terms
        ransomware_count = 0
        ransomware_matches = []

        for term in self.ransomware_terms:
            pattern = r'\b' + re.escape(term) + r'[a-z]*\b'
            matches = re.finditer(pattern, text)

            for match in matches:
                ransomware_count += 1
                ransomware_matches.append(match.group(0))

        results['ransomware_specific'] = {
            'count': ransomware_count,
            'matches': Counter(ransomware_matches).most_common()
        }

        self.framing_results[doc_id] = results
        return results

    def find_ransomware_in_articles(self, doc_id, articles):
        """Find articles that reference ransomware concepts."""
        if doc_id not in self.documents:
            print(f"Unknown document ID: {doc_id}")
            return {}

        ransomware_articles = {}

        for article_num, article_data in articles.items():
            content = article_data['content'].lower()
            title = article_data['title'].lower()

            # Check for ransomware-related terms
            ransomware_mentions = []

            for term in self.ransomware_terms:
                pattern = r'\b' + re.escape(term) + r'[a-z]*\b'
                matches = re.finditer(pattern, content)

                for match in matches:
                    ransomware_mentions.append({
                        'term': match.group(0),
                        'context': self._get_context(content, match.start(), 50)
                    })

                # Also check the title
                matches = re.finditer(pattern, title)
                for match in matches:
                    ransomware_mentions.append({
                        'term': match.group(0),
                        'context': f"TITLE: {title}"
                    })

            if ransomware_mentions:
                ransomware_articles[article_num] = {
                    'title': article_data['title'],
                    'mentions': ransomware_mentions
                }

        self.articles_references[doc_id] = ransomware_articles
        return ransomware_articles

    def find_ransomware_in_recitals(self, doc_id, recitals):
        """Find recitals that reference ransomware concepts."""
        if doc_id not in self.documents:
            print(f"Unknown document ID: {doc_id}")
            return {}

        ransomware_recitals = {}

        for recital_num, content in recitals.items():
            content_lower = content.lower()

            # Check for ransomware-related terms
            ransomware_mentions = []

            for term in self.ransomware_terms:
                pattern = r'\b' + re.escape(term) + r'[a-z]*\b'
                matches = re.finditer(pattern, content_lower)

                for match in matches:
                    ransomware_mentions.append({
                        'term': match.group(0),
                        'context': self._get_context(content_lower, match.start(), 50)
                    })

            if ransomware_mentions:
                ransomware_recitals[recital_num] = {
                    'content': content,
                    'mentions': ransomware_mentions
                }

        self.recitals_references[doc_id] = ransomware_recitals
        return ransomware_recitals

    def analyze_temporal_evolution(self):
        """Analyze how ransomware framing has evolved over time across documents."""
        if not self.framing_results:
            print("Run analyze_framing() for all documents first")
            return {}

        evolution = {
            'by_category': {},
            'by_document': {},
            'ransomware_specific': {}
        }

        # Sort documents by year
        sorted_docs = sorted(self.documents.items(), key=lambda x: x[1]['year'])

        # Analyze evolution by category
        for category in self.framing_categories:
            category_evolution = []

            for doc_id, doc_info in sorted_docs:
                if doc_id in self.framing_results:
                    doc_result = self.framing_results[doc_id]

                    if category in doc_result:
                        category_evolution.append({
                            'document': doc_id,
                            'year': doc_info['year'],
                            'count': doc_result[category]['count'],
                            'top_terms': doc_result[category]['matches'][:5] if doc_result[category]['matches'] else []
                        })

            evolution['by_category'][category] = category_evolution

        # Analyze ransomware-specific evolution
        ransomware_evolution = []

        for doc_id, doc_info in sorted_docs:
            if doc_id in self.framing_results:
                doc_result = self.framing_results[doc_id]

                if 'ransomware_specific' in doc_result:
                    ransomware_evolution.append({
                        'document': doc_id,
                        'year': doc_info['year'],
                        'count': doc_result['ransomware_specific']['count'],
                        'top_terms': doc_result['ransomware_specific']['matches'][:5] if doc_result['ransomware_specific']['matches'] else []
                    })

        evolution['ransomware_specific'] = ransomware_evolution

        # Analyze by document and year
        for doc_id, doc_info in sorted_docs:
            if doc_id in self.framing_results:
                doc_result = self.framing_results[doc_id]
                doc_evolution = {}

                for category in self.framing_categories:
                    if category in doc_result:
                        doc_evolution[category] = {
                            'count': doc_result[category]['count'],
                            'top_terms': doc_result[category]['matches'][:5] if doc_result[category]['matches'] else []
                        }

                evolution['by_document'][doc_id] = {
                    'year': doc_info['year'],
                    'title': doc_info['title'],
                    'categories': doc_evolution
                }

        self.temporal_evolution = evolution
        return evolution

    def compare_documents(self):
        """Compare ransomware framing across documents."""
        if not self.framing_results:
            print("Run analyze_framing() for all documents first")
            return {}

        comparison = {
            'category_matrix': {},
            'term_overlap': {},
            'unique_framings': {}
        }

        # Create comparison matrix for categories
        categories = list(self.framing_categories.keys()) + ['ransomware_specific']
        documents = list(self.documents.keys())

        category_matrix = pd.DataFrame(0, index=documents, columns=categories)

        for doc_id in documents:
            if doc_id in self.framing_results:
                for category in categories:
                    if category in self.framing_results[doc_id]:
                        category_matrix.loc[doc_id, category] = self.framing_results[doc_id][category]['count']

        comparison['category_matrix'] = category_matrix

        # Analyze term overlap
        for category in categories:
            term_matrix = {}

            for doc_id in documents:
                if doc_id in self.framing_results and category in self.framing_results[doc_id]:
                    term_matrix[doc_id] = [term for term, count in self.framing_results[doc_id][category]['matches']]

            # Find common terms across documents
            common_terms = set()
            if term_matrix:
                all_terms = list(term_matrix.values())
                if all_terms:
                    common_terms = set(all_terms[0])
                    for terms in all_terms[1:]:
                        common_terms = common_terms.intersection(set(terms))

            # Find unique terms for each document
            unique_terms = {}
            for doc_id, terms in term_matrix.items():
                other_docs_terms = set()
                for other_doc, other_terms in term_matrix.items():
                    if other_doc != doc_id:
                        other_docs_terms.update(other_terms)

                unique_terms[doc_id] = list(set(terms) - other_docs_terms)

            comparison['term_overlap'][category] = {
                'common_terms': list(common_terms),
                'unique_terms': unique_terms
            }

        # Identify unique framings
        for doc_id in documents:
            if doc_id in self.framing_results:
                unique_framings = {}

                for category in categories:
                    if category in self.framing_results[doc_id]:
                        # Get terms that are unique to this document in this category
                        doc_terms = {term for term, _ in self.framing_results[doc_id][category]['matches']}
                        other_terms = set()

                        for other_doc in documents:
                            if other_doc != doc_id and other_doc in self.framing_results and category in self.framing_results[other_doc]:
                                other_terms.update({term for term, _ in self.framing_results[other_doc][category]['matches']})

                        unique_terms = doc_terms - other_terms
                        if unique_terms:
                            unique_framings[category] = list(unique_terms)

                comparison['unique_framings'][doc_id] = unique_framings

        self.cross_document_comparison = comparison
        return comparison

    def visualize_framing_distribution(self):
        """Visualize the distribution of framing categories across documents."""
        if not self.cross_document_comparison or 'category_matrix' not in self.cross_document_comparison:
            print("Run compare_documents() first")
            return

        category_matrix = self.cross_document_comparison['category_matrix']

        # Create a heatmap
        plt.figure(figsize=(12, 8))
        sns.heatmap(category_matrix, annot=True, cmap='YlGnBu', fmt='g')
        plt.title('Distribution of Framing Categories Across Documents')
        plt.ylabel('Document')
        plt.xlabel('Category')
        plt.tight_layout()
        plt.show()

        # Create a grouped bar chart
        category_matrix.plot(kind='bar', figsize=(14, 8))
        plt.title('Distribution of Framing Categories Across Documents')
        plt.ylabel('Frequency')
        plt.xlabel('Document')
        plt.legend(title='Category')
        plt.tight_layout()
        plt.show()

    def visualize_temporal_evolution(self):
        """Visualize the temporal evolution of ransomware framing."""
        if not self.temporal_evolution or 'ransomware_specific' not in self.temporal_evolution:
            print("Run analyze_temporal_evolution() first")
            return

        # Extract data for visualization
        years = [item['year'] for item in self.temporal_evolution['ransomware_specific']]
        counts = [item['count'] for item in self.temporal_evolution['ransomware_specific']]
        documents = [item['document'] for item in self.temporal_evolution['ransomware_specific']]

        # Create a line plot
        plt.figure(figsize=(12, 6))
        plt.plot(years, counts, marker='o', linestyle='-', linewidth=2, markersize=10)

        # Add labels for each point
        for i, doc in enumerate(documents):
            plt.annotate(doc, (years[i], counts[i]), textcoords="offset points",
                         xytext=(0, 10), ha='center')

        plt.title('Evolution of Ransomware-Specific Terms Over Time')
        plt.xlabel('Year')
        plt.ylabel('Frequency')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()

        # Create a category evolution plot
        plt.figure(figsize=(14, 8))

        for category in self.framing_categories:
            if category in self.temporal_evolution['by_category']:
                category_data = self.temporal_evolution['by_category'][category]
                years = [item['year'] for item in category_data]
                counts = [item['count'] for item in category_data]

                if years and counts:
                    plt.plot(years, counts, marker='o', linestyle='-', linewidth=2, markersize=8, label=category)

        plt.title('Evolution of Framing Categories Over Time')
        plt.xlabel('Year')
        plt.ylabel('Frequency')
        plt.legend(title='Category')
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.show()

    def generate_network_analysis(self):
        """Generate a network analysis of framing terms and documents."""
        if not self.framing_results:
            print("Run analyze_framing() for all documents first")
            return

        # Create a graph
        G = nx.Graph()

        # Add document nodes
        for doc_id, doc_info in self.documents.items():
            if doc_id in self.framing_results:
                G.add_node(doc_id, type='document', year=doc_info['year'], label=doc_id)

        # Add category nodes and connect to documents
        for category in self.framing_categories:
            G.add_node(category, type='category', label=category)

            for doc_id in self.framing_results:
                if category in self.framing_results[doc_id]:
                    count = self.framing_results[doc_id][category]['count']
                    if count > 0:
                        G.add_edge(doc_id, category, weight=count)

        # Add key term nodes and connect to categories
        added_terms = set()
        for category, terms in self.framing_categories.items():
            for term in terms[:5]:  # Add top 5 terms per category
                if term not in added_terms:
                    G.add_node(term, type='term', label=term)
                    added_terms.add(term)
                    G.add_edge(category, term)

        # Visualize the network
        plt.figure(figsize=(14, 10))

        # Define node positions
        pos = nx.spring_layout(G, k=0.3, iterations=50)

        # Draw nodes by type
        document_nodes = [node for node, attrs in G.nodes(data=True) if attrs.get('type') == 'document']
        category_nodes = [node for node, attrs in G.nodes(data=True) if attrs.get('type') == 'category']
        term_nodes = [node for node, attrs in G.nodes(data=True) if attrs.get('type') == 'term']

        nx.draw_networkx_nodes(G, pos, nodelist=document_nodes, node_color='skyblue',
                               node_size=800, alpha=0.8, label='Documents')
        nx.draw_networkx_nodes(G, pos, nodelist=category_nodes, node_color='lightgreen',
                               node_size=600, alpha=0.8, label='Categories')
        nx.draw_networkx_nodes(G, pos, nodelist=term_nodes, node_color='lightcoral',
                               node_size=400, alpha=0.8, label='Terms')

        # Draw edges with weights
        edges = [(u, v) for u, v, d in G.edges(data=True) if 'weight' in d]
        weights = [G[u][v]['weight'] for u, v in edges]

        nx.draw_networkx_edges(G, pos, edgelist=edges, width=[w/5 for w in weights],
                               alpha=0.7, edge_color='gray')

        other_edges = [(u, v) for u, v in G.edges() if 'weight' not in G[u][v]]
        nx.draw_networkx_edges(G, pos, edgelist=other_edges, width=1,
                               alpha=0.5, edge_color='lightgray')

        # Draw labels
        nx.draw_networkx_labels(G, pos, font_size=10)

        plt.title('Network Analysis of Ransomware Framing')
        plt.legend()
        plt.axis('off')
        plt.tight_layout()
        plt.show()

        return G

    def generate_summary_report(self):
        """Generate a comprehensive summary report of the analysis."""
        if not self.framing_results or not self.temporal_evolution or not self.cross_document_comparison:
            print("Run all analysis methods first")
            return

        report = []

        # Document overview
        report.append("# RANSOMWARE FRAMING ANALYSIS REPORT")
        report.append("\n## 1. Document Overview")

        for doc_id, doc_info in self.documents.items():
            report.append(f"\n### {doc_info['title']} ({doc_info['year']})")
            report.append(f"- Type: {doc_info['type']}")
            report.append(f"- Issuing Body: {doc_info['body']}")

            if doc_id in self.framing_results:
                # Add ransomware-specific mentions
                if 'ransomware_specific' in self.framing_results[doc_id]:
                    spec_count = self.framing_results[doc_id]['ransomware_specific']['count']
                    report.append(f"- Ransomware-specific mentions: {spec_count}")

                    if spec_count > 0:
                        top_terms = self.framing_results[doc_id]['ransomware_specific']['matches'][:5]
                        terms_str = ", ".join([f"{term} ({count})" for term, count in top_terms])
                        report.append(f"- Top ransomware terms: {terms_str}")

        # Framing categories analysis
        report.append("\n## 2. Framing Categories Analysis")

        for category in self.framing_categories:
            report.append(f"\n### {category.capitalize()} Framing")
            report.append(f"Definition: Framing ransomware in terms of {', '.join(self.framing_categories[category][:5])}...")

            for doc_id, doc_info in self.documents.items():
                if doc_id in self.framing_results and category in self.framing_results[doc_id]:
                    count = self.framing_results[doc_id][category]['count']
                    report.append(f"\n#### {doc_info['title']} ({doc_info['year']})")
                    report.append(f"- Frequency: {count}")

                    if count > 0:
                        top_terms = self.framing_results[doc_id][category]['matches'][:5]
                        terms_str = ", ".join([f"{term} ({count})" for term, count in top_terms])
                        report.append(f"- Top terms: {terms_str}")

        # Temporal evolution
        report.append("\n## 3. Temporal Evolution of Ransomware Framing")
        report.append("\n### Evolution of Ransomware-Specific Terms")

        for item in self.temporal_evolution['ransomware_specific']:
            doc_id = item['document']
            year = item['year']
            count = item['count']

            report.append(f"\n#### {self.documents[doc_id]['title']} ({year})")
            report.append(f"- Frequency: {count}")

            if count > 0 and item['top_terms']:
                terms_str = ", ".join([f"{term} ({count})" for term, count in item['top_terms']])
                report.append(f"- Top terms: {terms_str}")

        report.append("\n### Evolution by Framing Category")

        for category in self.framing_categories:
            if category in self.temporal_evolution['by_category']:
                report.append(f"\n#### {category.capitalize()} Framing")

                for item in self.temporal_evolution['by_category'][category]:
                    doc_id = item['document']
                    year = item['year']
                    count = item['count']

                    report.append(f"- {self.documents[doc_id]['title']} ({year}): {count} mentions")

        # References in articles and recitals
        report.append("\n## 4. Ransomware References in Legal Provisions")

        for doc_id, doc_info in self.documents.items():
            report.append(f"\n### {doc_info['title']} ({doc_info['year']})")

            # Articles references
            if doc_id in self.articles_references and self.articles_references[doc_id]:
                report.append("\n#### Articles")

                for article_num, article_data in self.articles_references[doc_id].items():
                    report.append(f"\n- Article {article_num}: {article_data['title']}")

                    for mention in article_data['mentions'][:3]:  # Show up to 3 mentions
                        report.append(f"  - Term: {mention['term']}")
                        report.append(f"    Context: \"{mention['context']}\"")

            # Recitals references
            if doc_id in self.recitals_references and self.recitals_references[doc_id]:
                report.append("\n#### Recitals")

                for recital_num, recital_data in self.recitals_references[doc_id].items():
                    report.append(f"\n- Recital {recital_num}")

                    for mention in recital_data['mentions'][:3]:  # Show up to 3 mentions
                        report.append(f"  - Term: {mention['term']}")
                        report.append(f"    Context: \"{mention['context']}\"")

        # Cross-document comparison
        report.append("\n## 5. Cross-Document Comparison")

        # Common framing terms
        report.append("\n### Common Framing Terms Across Documents")

        for category in self.framing_categories:
            if (category in self.cross_document_comparison['term_overlap'] and
                self.cross_document_comparison['term_overlap'][category]['common_terms']):

                common_terms = self.cross_document_comparison['term_overlap'][category]['common_terms']
                report.append(f"\n#### {category.capitalize()} Framing")
                report.append(f"- Common terms: {', '.join(common_terms)}")

        # Unique framings
        report.append("\n### Unique Framings by Document")

        for doc_id, unique_framings in self.cross_document_comparison['unique_framings'].items():
            if unique_framings:
                report.append(f"\n#### {self.documents[doc_id]['title']} ({self.documents[doc_id]['year']})")

                for category, terms in unique_framings.items():
                    report.append(f"- {category.capitalize()} framing: {', '.join(terms)}")

        # Conclusion
        report.append("\n## 6. Conclusion: Evolutionary Trends in Ransomware Framing")

        # Calculate overall trends
        earliest_doc = min(self.documents.items(), key=lambda x: x[1]['year'])
        latest_doc = max(self.documents.items(), key=lambda x: x[1]['year'])

        earliest_id = earliest_doc[0]
        latest_id = latest_doc[0]

        report.append(f"\nFrom {earliest_doc[1]['year']} to {latest_doc[1]['year']}, the framing of ransomware has evolved significantly across international and EU cybercrime instruments.")

        # Compare ransomware-specific framings
        if (earliest_id in self.framing_results and 'ransomware_specific' in self.framing_results[earliest_id] and
            latest_id in self.framing_results and 'ransomware_specific' in self.framing_results[latest_id]):

            earliest_count = self.framing_results[earliest_id]['ransomware_specific']['count']
            latest_count = self.framing_results[latest_id]['ransomware_specific']['count']

            report.append(f"\n### Overall Emphasis on Ransomware")
            report.append(f"- {earliest_doc[1]['title']} ({earliest_doc[1]['year']}): {earliest_count} mentions")
            report.append(f"- {latest_doc[1]['title']} ({latest_doc[1]['year']}): {latest_count} mentions")

            if latest_count > earliest_count:
                report.append(f"\nThis represents a {latest_count / earliest_count if earliest_count > 0 else 'significant'} increase in explicit focus on ransomware.")

        # Compare framing categories
        report.append("\n### Shifts in Framing Categories")

        for category in self.framing_categories:
            if (earliest_id in self.framing_results and category in self.framing_results[earliest_id] and
                latest_id in self.framing_results and category in self.framing_results[latest_id]):

                earliest_count = self.framing_results[earliest_id][category]['count']
                latest_count = self.framing_results[latest_id][category]['count']

                report.append(f"\n#### {category.capitalize()} Framing")
                report.append(f"- {earliest_doc[1]['title']} ({earliest_doc[1]['year']}): {earliest_count} mentions")
                report.append(f"- {latest_doc[1]['title']} ({latest_doc[1]['year']}): {latest_count} mentions")

                if latest_count > earliest_count:
                    report.append(f"This represents an increase in {category} framing.")
                elif latest_count < earliest_count:
                    report.append(f"This represents a decrease in {category} framing.")

        return "\n".join(report)

    def _get_context(self, text, position, context_size):
        """Helper method to get context around a match position."""
        start = max(0, position - context_size)
        end = min(len(text), position + context_size)

        context = text[start:end]

        # Add ellipsis if context is truncated
        if start > 0:
            context = "..." + context
        if end < len(text):
            context = context + "..."

        return context

# Example usage function
def run_analysis(budapest_text, attacks_directive_text, ransomware_guidance_text, nis2_text):
    """Run a complete analysis on the provided document texts."""

    # Initialize the analyzer
    analyzer = RansomwareFrameworkAnalyzer()

    # Load documents
    analyzer.load_document('budapest', budapest_text)
    analyzer.load_document('directive_attacks', attacks_directive_text)
    analyzer.load_document('ransomware_guidance', ransomware_guidance_text)
    analyzer.load_document('nis2', nis2_text)

    # Extract articles and recitals
    results = {}

    for doc_id in analyzer.documents:
        # Extract articles
        articles = analyzer.extract_articles(doc_id)
        results[f'{doc_id}_articles'] = articles

        # Extract recitals
        recitals = analyzer.extract_recitals(doc_id)
        results[f'{doc_id}_recitals'] = recitals

        # Analyze framing
        framing = analyzer.analyze_framing(doc_id)
        results[f'{doc_id}_framing'] = framing

        # Find ransomware references in articles
        ransomware_articles = analyzer.find_ransomware_in_articles(doc_id, articles)
        results[f'{doc_id}_ransomware_articles'] = ransomware_articles

        # Find ransomware references in recitals
        ransomware_recitals = analyzer.find_ransomware_in_recitals(doc_id, recitals)
        results[f'{doc_id}_ransomware_recitals'] = ransomware_recitals

    # Analyze temporal evolution
    evolution = analyzer.analyze_temporal_evolution()
    results['temporal_evolution'] = evolution

    # Compare documents
    comparison = analyzer.compare_documents()
    results['cross_document_comparison'] = comparison

    # Generate summary report
    report = analyzer.generate_summary_report()
    results['report'] = report

    # Return the analyzer for further use
    return analyzer, results

# Additional utility functions for Google Colab usage
def load_file_from_colab(file_path):
    """Load a file that has been uploaded to Colab."""
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def extract_text_sections(text, section_name, max_sections=5):
    """Extract specific sections from a document."""
    sections = []
    pattern = rf'{re.escape(section_name)}\s+(\d+)[:\s]+(.*?)(?=\n\s*{re.escape(section_name)}\s+\d+:|$)'
    matches = re.finditer(pattern, text, re.DOTALL)

    for i, match in enumerate(matches):
        if i >= max_sections:
            break

        section_num = match.group(1)
        section_content = match.group(2).strip()
        sections.append({
            'number': section_num,
            'content': section_content
        })

    return sections

def find_key_terms_in_context(text, terms, context_size=50):
    """Find key terms in context within a document."""
    results = []

    for term in terms:
        pattern = r'\b' + re.escape(term) + r'[a-z]*\b'
        matches = re.finditer(pattern, text.lower())

        for match in matches:
            start = max(0, match.start() - context_size)
            end = min(len(text), match.end() + context_size)

            context = text[start:end]
            if start > 0:
                context = "..." + context
            if end < len(text):
                context = context + "..."

            results.append({
                'term': match.group(0),
                'position': match.start(),
                'context': context
            })

    return results

def generate_topic_model(documents, num_topics=5, num_words=10):
    """Generate a topic model from the documents."""
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.feature_extraction.text import CountVectorizer

    # Combine all documents
    all_texts = []
    for doc_id, doc_info in documents.items():
        if doc_info['text']:
            all_texts.append(doc_info['text'])

    # Create a document-term matrix
    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
    dtm = vectorizer.fit_transform(all_texts)

    # Fit LDA model
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(dtm)

    # Get feature names
    feature_names = vectorizer.get_feature_names_out()

    # Extract topics
    topics = []
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[:-num_words-1:-1]
        top_words = [feature_names[i] for i in top_words_idx]
        topics.append({
            'topic_id': topic_idx,
            'words': top_words
        })

    return topics, lda, vectorizer, dtm

# Step 2: Upload the documents to Google Colab
from google.colab import files
print("Please upload the following documents:")
print("1. CETS_185.pdf (Budapest Convention)")
print("2. DIRECTIVE 2013 40 EU.pdf (Directive on Attacks Against Information Systems)")
print("3. T-CY Ransomware guidance note.pdf (T-CY Guidance Note #12)")
print("4. DIRECTIVE (EU) 2022 2555.pdf (NIS 2 Directive)")
print("Click on 'Choose Files' below and select all four documents.")

uploaded = files.upload()

# Step 3: Extract text from PDFs
!pip install PyPDF2
import PyPDF2

def extract_text_from_pdf(file_path):
    with open(file_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()
    return text

# Extract text from uploaded documents
file_names = list(uploaded.keys())
print("\nUploaded files:", file_names)

# Try to find the correct files based on naming patterns
budapest_file = next((f for f in file_names if 'cets' in f.lower() or 'buda' in f.lower() or '185' in f), None)
attacks_file = next((f for f in file_names if '2013' in f or '40' in f or 'attack' in f.lower()), None)
guidance_file = next((f for f in file_names if 'guid' in f.lower() or 'ransom' in f.lower() or 't-cy' in f.lower()), None)
nis2_file = next((f for f in file_names if 'nis' in f.lower() or '2555' in f or '2022' in f), None)

# Extract text from each file
if budapest_file:
    budapest_text = extract_text_from_pdf(budapest_file)
    print(f"Budapest Convention: {len(budapest_text)} characters extracted")
else:
    print("Budapest Convention file not found. Please check uploaded files.")
    budapest_text = ""

if attacks_file:
    attacks_directive_text = extract_text_from_pdf(attacks_file)
    print(f"Directive on Attacks: {len(attacks_directive_text)} characters extracted")
else:
    print("Directive on Attacks file not found. Please check uploaded files.")
    attacks_directive_text = ""

if guidance_file:
    ransomware_guidance_text = extract_text_from_pdf(guidance_file)
    print(f"Ransomware Guidance: {len(ransomware_guidance_text)} characters extracted")
else:
    print("Ransomware Guidance file not found. Please check uploaded files.")
    ransomware_guidance_text = ""

if nis2_file:
    nis2_text = extract_text_from_pdf(nis2_file)
    print(f"NIS 2 Directive: {len(nis2_text)} characters extracted")
else:
    print("NIS 2 Directive file not found. Please check uploaded files.")
    nis2_text = ""

# Step 4: Run the analysis
analyzer, results = run_analysis(budapest_text, attacks_directive_text, ransomware_guidance_text, nis2_text)

# Step 5: Visualize the results
print("\n\nGenerating visualizations...\n")

# 5.1: Visualize framing distribution
analyzer.visualize_framing_distribution()

# 5.2: Visualize temporal evolution
analyzer.visualize_temporal_evolution()

# 5.3: Generate network analysis
analyzer.generate_network_analysis()

# Step 6: Display the summary report
print("\n\n" + "="*80)
print("RANSOMWARE FRAMING ANALYSIS REPORT")
print("="*80 + "\n")
print(results['report'])

# Step 7: Additional focused analysis
print("\n\n" + "="*80)
print("DETAILED RANSOMWARE FRAMING ANALYSIS")
print("="*80 + "\n")

# 7.1: Analyze framing categories in depth
framing_categories = analyzer.framing_categories.keys()
category_data = {}

for doc_id, doc_info in analyzer.documents.items():
    if doc_id in analyzer.framing_results:
        doc_data = {}
        for category in framing_categories:
            if category in analyzer.framing_results[doc_id]:
                doc_data[category] = analyzer.framing_results[doc_id][category]['count']
        category_data[doc_id] = doc_data

df_categories = pd.DataFrame(category_data).T
df_categories.index = [f"{analyzer.documents[doc_id]['title']} ({analyzer.documents[doc_id]['year']})" for doc_id in df_categories.index]

print("Framing Categories Count by Document")
print(df_categories)
print("\n")

# Normalize by document to see relative importance
df_normalized = df_categories.div(df_categories.sum(axis=1), axis=0) * 100
print("Relative Importance of Framing Categories (%)")
print(df_normalized.round(2))

# Plot the normalized data
plt.figure(figsize=(14, 10))
df_normalized.plot(kind='bar', stacked=True, figsize=(14, 8), colormap='viridis')
plt.title('Relative Importance of Framing Categories Across Documents')
plt.ylabel('Percentage (%)')
plt.xlabel('Document')
plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# 7.2: Analyze specific ransomware terms
ransomware_terms = analyzer.ransomware_terms
term_counts = {}

for doc_id, doc_info in analyzer.documents.items():
    if doc_info['text']:
        doc_counts = {}
        for term in ransomware_terms:
            pattern = r'\b' + re.escape(term) + r'[a-z]*\b'
            matches = re.finditer(pattern, doc_info['text'].lower())
            count = sum(1 for _ in matches)
            if count > 0:
                doc_counts[term] = count
        term_counts[doc_id] = doc_counts

# Create a DataFrame
df_terms = pd.DataFrame({
    doc_id: {term: term_counts.get(doc_id, {}).get(term, 0) for term in ransomware_terms}
    for doc_id in analyzer.documents
})

df_terms.index.name = 'Term'
df_terms.columns = [f"{analyzer.documents[doc_id]['title']} ({analyzer.documents[doc_id]['year']})" for doc_id in df_terms.columns]

# Display most common terms
print("\nTop Ransomware Terms by Document")
print(df_terms.sum().sort_values(ascending=False))

# Plot top 10 terms across all documents
top_terms = df_terms.sum(axis=1).sort_values(ascending=False).head(10).index.tolist()
df_top_terms = df_terms.loc[top_terms]

plt.figure(figsize=(14, 8))
df_top_terms.T.plot(kind='bar', figsize=(14, 8))
plt.title('Top 10 Ransomware Terms Across Documents')
plt.ylabel('Frequency')
plt.xlabel('Document')
plt.legend(title='Term')
plt.tight_layout()
plt.show()

# 7.3: Generate table of relevant articles and recitals
print("\nRelevant Articles and Recitals Mentioning Ransomware Concepts")

for doc_id, doc_info in analyzer.documents.items():
    print(f"\n{doc_info['title']} ({doc_info['year']})")

    # Articles
    if doc_id in analyzer.articles_references and analyzer.articles_references[doc_id]:
        print("\nRelevant Articles:")
        articles_data = []

        for article_num, article_data in analyzer.articles_references[doc_id].items():
            terms = ", ".join(set([mention['term'] for mention in article_data['mentions']]))
            articles_data.append({
                'Article': article_num,
                'Title': article_data['title'],
                'Ransomware Terms': terms,
                'Mentions': len(article_data['mentions'])
            })

        df_articles = pd.DataFrame(articles_data)
        df_articles = df_articles.sort_values('Mentions', ascending=False)
        print(df_articles)

    # Recitals
    if doc_id in analyzer.recitals_references and analyzer.recitals_references[doc_id]:
        print("\nRelevant Recitals:")
        recitals_data = []

        for recital_num, recital_data in analyzer.recitals_references[doc_id].items():
            terms = ", ".join(set([mention['term'] for mention in recital_data['mentions']]))
            recitals_data.append({
                'Recital': recital_num,
                'Ransomware Terms': terms,
                'Mentions': len(recital_data['mentions'])
            })

        df_recitals = pd.DataFrame(recitals_data)
        df_recitals = df_recitals.sort_values('Mentions', ascending=False)
        print(df_recitals)

# 7.4: Topic modeling across documents
print("\nTopic Modeling Across Documents")
topics, lda, vectorizer, dtm = generate_topic_model(analyzer.documents)

for i, topic in enumerate(topics):
    print(f"\nTopic {i+1}: {', '.join(topic['words'])}")

# 7.5: Evolutionary trends visualization
print("\nEvolutionary Trends in Ransomware Framing")

# Create a DataFrame with years and category counts
trend_data = []

for doc_id, doc_info in sorted(analyzer.documents.items(), key=lambda x: x[1]['year']):
    if doc_id in analyzer.framing_results:
        row = {'document': doc_id, 'year': doc_info['year'], 'title': doc_info['title']}

        for category in analyzer.framing_categories:
            if category in analyzer.framing_results[doc_id]:
                row[category] = analyzer.framing_results[doc_id][category]['count']

        if 'ransomware_specific' in analyzer.framing_results[doc_id]:
            row['ransomware_specific'] = analyzer.framing_results[doc_id]['ransomware_specific']['count']

        trend_data.append(row)

df_trends = pd.DataFrame(trend_data)

# Plot the evolution of all categories
plt.figure(figsize=(14, 8))
categories = list(analyzer.framing_categories.keys()) + ['ransomware_specific']

for category in categories:
    if category in df_trends.columns:
        plt.plot(df_trends['year'], df_trends[category], marker='o', label=category)

plt.title('Evolution of Framing Categories (2001-2022)')
plt.xlabel('Year')
plt.ylabel('Frequency')
plt.legend(title='Category')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Step 8: Download the complete report
from google.colab import files
import io

report_file = io.StringIO()
report_file.write(results['report'])
report_file.seek(0)

with open('ransomware_analysis_report.md', 'w') as f:
    f.write(results['report'])

files.download('ransomware_analysis_report.md')
print("\nReport download initiated. Check your downloads folder for 'ransomware_analysis_report.md'")